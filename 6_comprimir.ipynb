{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484a6ac7-23ef-4ec1-be90-f275f5107955",
   "metadata": {},
   "source": [
    "#### Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961799d-d338-4aa7-bcf7-e3d931c8f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Análisis de Datos y Numérico ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualización de Datos ---\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import font_manager\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "\n",
    "# --- Análisis de Redes y Grafos ---\n",
    "import networkx as nx\n",
    "from networkx import florentine_families_graph\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "# --- Utilidades Generales y de Jupyter ---\n",
    "import colorsys\n",
    "from random import random\n",
    "from typing import Dict\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "input_file_path = r\"RUTA\\obligaciones_combinadas_expandidas_ajustadas.xlsx\"\n",
    "\n",
    "df = pd.read_excel(input_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f57c95-b1f3-4579-9185-2f12ada524d3",
   "metadata": {},
   "source": [
    "#### Corregir errores de Fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e106c-ee3b-4782-90b1-fbdb38805591",
   "metadata": {},
   "outputs": [],
   "source": [
    "condicion = df['fuente'].str.contains('codigo de aguas', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto con fuerza de ley'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 1122\n",
    "df.loc[condicion, 'año_fuente'] = 1981\n",
    "\n",
    "condicion = df['fuente'].str.contains('ordenanza', case=False, na=False) \n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 47\n",
    "df.loc[condicion, 'año_fuente'] = 1992\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('urbanismo', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto con fuerza de ley'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 458\n",
    "df.loc[condicion, 'año_fuente'] = 1975\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('pesca', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'ley'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 18892\n",
    "df.loc[condicion, 'año_fuente'] = 1989\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('seguridad minera', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo' \n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 132\n",
    "df.loc[condicion, 'año_fuente'] = 2002\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('residuos peligrosos', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 148\n",
    "df.loc[condicion, 'año_fuente'] = 2003\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('bosques', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo' \n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 4363\n",
    "df.loc[condicion, 'año_fuente'] = 1931\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('cargas', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 298\n",
    "df.loc[condicion, 'año_fuente'] = 1994\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('corrientes fuertes', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] =  4188\n",
    "df.loc[condicion, 'año_fuente'] = 1955\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('emisiones', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 1\n",
    "df.loc[condicion, 'año_fuente'] = 2013\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('fuego', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 276\n",
    "df.loc[condicion, 'año_fuente'] = 1980\n",
    "\n",
    "condicion = df['numero_fuente_ajustado'].str.contains('transporte', case=False, na=False)\n",
    "df.loc[condicion, 'fuente'] = 'decreto supremo'\n",
    "df.loc[condicion, 'numero_fuente_ajustado'] = 75\n",
    "df.loc[condicion, 'año_fuente'] = 1987"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195245a9-e8eb-4e17-8e50-5641c80206d6",
   "metadata": {},
   "source": [
    "#### UN Solo N°PAS por cada RCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83530e44-753f-4073-8b01-5a28479e8070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    subset_cols = ['cell', 'numero_fuente_ajustado']\n",
    "    mask_pas = (df['seccion'] == 'pas') & (df['numero_fuente_ajustado'] != \"No identificado\") & (df['numero_fuente_ajustado'].notna())\n",
    "    \n",
    "    duplicated_rows = df[mask_pas & df.duplicated(subset=subset_cols, keep=False)]\n",
    "\n",
    "    if duplicated_rows.empty:\n",
    "        print(\"No se encontraron filas duplicadas para modificar. El archivo de salida no fue creado.\")\n",
    "    else:\n",
    "        print(f\"Se encontraron {len(duplicated_rows)} filas involucradas en duplicados. Analizando grupos...\\n\")\n",
    "        \n",
    "        indices_to_modify = []\n",
    "        duplicated_groups = duplicated_rows[subset_cols].drop_duplicates().values\n",
    "        for cell, nfa in duplicated_groups:\n",
    "            group_df = duplicated_rows[(duplicated_rows['cell'] == cell) & (duplicated_rows['numero_fuente_ajustado'] == nfa)]\n",
    "            priority_condition = (group_df['numero_fuente'].astype(str) != group_df['numero_fuente_ajustado'].astype(str)) & group_df['numero_fuente'].notna()\n",
    "            priority_rows = group_df[priority_condition]\n",
    "            non_priority_rows = group_df[~priority_condition]\n",
    "\n",
    "            kept_indices = []\n",
    "            modified_indices_in_group = []\n",
    "            reason = \"\"\n",
    "\n",
    "            if not priority_rows.empty and not non_priority_rows.empty:\n",
    "                reason = \"Prioridad (numero_fuente != ajustado)\"\n",
    "                kept_indices.extend(priority_rows.index.tolist())\n",
    "                modified_indices_in_group.extend(non_priority_rows.index.tolist())\n",
    "                \n",
    "            else:\n",
    "                reason = \"Mismo grupo (keep='first')\"\n",
    "                all_indices = group_df.index.tolist()\n",
    "                kept_indices.append(all_indices[0])\n",
    "                modified_indices_in_group.extend(all_indices[1:])\n",
    "\n",
    "            indices_to_modify.extend(modified_indices_in_group)\n",
    "            print(f\"Grupo Duplicado: cell='{cell}', numero_fuente_ajustado='{nfa}'\")\n",
    "            print(f\"  └─ Razón de la decisión: {reason}\")\n",
    "            kept_values = df.loc[kept_indices, 'numero_fuente_ajustado'].unique().tolist()\n",
    "            modified_values = df.loc[modified_indices_in_group, 'numero_fuente_ajustado'].unique().tolist()\n",
    "\n",
    "            if modified_indices_in_group:\n",
    "                pass\n",
    "\n",
    "        if indices_to_modify:\n",
    "            print(f\"\\nSe modificarán un total de {len(indices_to_modify)} filas.\")\n",
    "            df.loc[indices_to_modify, 'numero_fuente_ajustado'] = 'No identificado - modificado'\n",
    "        else:\n",
    "             print(\"\\nAunque se encontraron grupos duplicados, la lógica no requirió modificar ninguna fila.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se pudo encontrar el archivo en la ruta especificada:\\n{input_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344e556-deeb-49c9-ae3a-81fdc68ab615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['seccion'] == 'pas', 'fuente'] = 'permisos ambientales sectoriales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89007d6d-8e0c-4492-876d-a8d13715e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_years = df['numero_fuente_completo'].str.extract(r'\\/(\\d{4})$', expand=False).astype(float)\n",
    "valid_years = extracted_years.where((extracted_years >= 1900) & (extracted_years <= 2025))\n",
    "diferencia_absoluta = (df['año_fuente'] - valid_years).abs()\n",
    "condicion_final = (valid_years.notna()) & ((diferencia_absoluta > 1) | (df['año_fuente'].isna()))\n",
    "df.loc[condicion_final, 'año_fuente'] = valid_years[condicion_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33a6bf-1836-4397-92e2-aaa9a17d30ba",
   "metadata": {},
   "source": [
    "### Corregir Duplicadas y las con más de 1 fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe4d2d-bf78-459b-927a-aa7916a15fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuentes_a_limpiar = [\"Compromiso ambiental voluntario\", \"decisión de autoridad\"]\n",
    "df.loc[df['fuente'].isin(fuentes_a_limpiar), 'numero_fuente_ajustado'] = \"\"\n",
    "df.loc[df['año_fuente'].isin(fuentes_a_limpiar), 'numero_fuente_ajustado'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d838ec-1359-464c-b3b2-aa0939c3e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['obligacion_id', 'fuente', 'numero_fuente_ajustado'], keep='first')\n",
    "def aplicar_logica_filtrado(grupo):\n",
    "    if len(grupo) <= 1:\n",
    "        return grupo\n",
    "\n",
    "    fuentes_presentes = set(grupo['fuente'])\n",
    "    if \"permisos ambientales sectoriales\" in fuentes_presentes:\n",
    "        return grupo[grupo['fuente'] == \"permisos ambientales sectoriales\"]\n",
    "\n",
    "    if \"Compromiso ambiental voluntario\" in fuentes_presentes and \"decisión de autoridad\" in fuentes_presentes:\n",
    "        return grupo[grupo['fuente'] == \"Compromiso ambiental voluntario\"]\n",
    "\n",
    "    if \"decisión de autoridad\" in fuentes_presentes:\n",
    "        otras_fuentes = fuentes_presentes - {\"decisión de autoridad\", \"Compromiso ambiental voluntario\", \"permisos ambientales sectoriales\"}\n",
    "        if otras_fuentes:\n",
    "            return grupo[grupo['fuente'] != \"decisión de autoridad\"]\n",
    "\n",
    "    return grupo\n",
    "\n",
    "df = df.groupby('obligacion_id', group_keys=False).apply(aplicar_logica_filtrado)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f33991-87c1-4cab-9fb8-97b116fe8310",
   "metadata": {},
   "source": [
    "### Corregir Otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ba052-06e0-47dd-96c1-4e691c5f9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_correcciones_normativas(df):\n",
    "    \"\"\"\n",
    "    Aplica correcciones masivas a los datos normativos según tabla de conversiones\n",
    "    Versión mejorada que maneja diferentes tipos de datos y formatos\n",
    "    \"\"\"\n",
    "    \n",
    "    def normalizar_fuente(fuente):\n",
    "        \"\"\"Normaliza el texto de la fuente para comparaciones consistentes\"\"\"\n",
    "        if pd.isna(fuente):\n",
    "            return ''\n",
    "        return str(fuente).lower().strip()\n",
    "    \n",
    "    def normalizar_numero(numero):\n",
    "        \"\"\"Normaliza el número para comparaciones consistentes\"\"\"\n",
    "        if pd.isna(numero):\n",
    "            return None\n",
    "        try:\n",
    "            # Convertir a int para comparación consistente\n",
    "            return int(float(str(numero).strip()))\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def normalizar_año(año):\n",
    "        \"\"\"Normaliza el año para comparaciones consistentes\"\"\"\n",
    "        if pd.isna(año):\n",
    "            return None\n",
    "        try:\n",
    "            # Convertir a int para comparación consistente\n",
    "            return int(float(str(año).strip()))\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    # Tabla de correcciones: (fuente_origen, numero_origen, año_origen) -> (fuente_destino, numero_destino, año_destino)\n",
    "    correcciones = [\n",
    "        # (fuente_origen, numero_origen, año_origen, fuente_destino, numero_destino, año_destino)\n",
    "        ('ley', 17798, 1972, 'decreto supremo', 400, 1977),\n",
    "        ('decreto supremo', 400, 1972, 'decreto supremo', 400, 1977),\n",
    "        ('decreto supremo', 149, 2006, 'decreto supremo', 80, 2004),\n",
    "        ('decreto con fuerza de ley', 4, 1994, 'decreto supremo', 4, 1994), \n",
    "        ('decreto supremo', 68, 2021, 'decreto supremo', 327, 1997),\n",
    "        ('decreto supremo', 327, 2021, 'decreto supremo', 327, 1997),\n",
    "        ('decreto supremo', 31, 2017, 'decreto supremo', 31, 2016),\n",
    "        ('decreto supremo', 23, 1926, 'decreto supremo', 236, 1926),\n",
    "        ('decreto supremo', 157, 2007, 'decreto supremo', 157, 2005),\n",
    "        ('decreto supremo', 72, 1985, 'decreto supremo', 132, 2002),\n",
    "        ('decreto supremo', 132, 1985, 'decreto supremo', 132, 2002),\n",
    "        ('decreto supremo', 1150, 1980, 'decreto supremo', 100, 2005),\n",
    "        ('decreto supremo', 100, 1980, 'decreto supremo', 100, 2005),\n",
    "        ('decreto ley', 93557, 1980, 'decreto ley', 3557, 1980),\n",
    "        ('decreto ley', 701, 1974, 'decreto ley', 2565, 1979),\n",
    "        ('decreto ley', 701, 1979, 'decreto ley', 2565, 1979),\n",
    "        ('ley', 15840, 1964, 'decreto con fuerza de ley', 850, 1997),\n",
    "        ('decreto supremo', 294, 1984, 'decreto con fuerza de ley', 850, 1997),\n",
    "        ('decreto con fuerza de ley', 850, 1984, 'decreto con fuerza de ley', 850, 1997),\n",
    "        ('decreto con fuerza de ley', 850, 1964, 'decreto con fuerza de ley', 850, 1997),\n",
    "        ('ley', 20724, 2014, 'decreto con fuerza de ley', 725, 1967),\n",
    "        ('decreto supremo', 90, 2010, 'decreto con fuerza de ley', 725, 1967),\n",
    "        ('decreto supremo', 90, 2011, 'decreto con fuerza de ley', 725, 1967),\n",
    "        ('ley', 20443, 2010, 'decreto con fuerza de ley', 458, 1975),\n",
    "        ('decreto con fuerza de ley', 1, 1982, 'decreto con fuerza de ley', 4, 2006),\n",
    "        ('decreto con fuerza de ley', 4, 1982, 'decreto con fuerza de ley', 4, 2006),\n",
    "        ('decreto con fuerza de ley', 4, 2007, 'decreto con fuerza de ley', 4, 2006), \n",
    "        ('decreto con fuerza de ley', 1, 2009, 'decreto con fuerza de ley', 1, 2007), \n",
    "        ('ley', 18290, 1984, 'decreto con fuerza de ley', 1, 2007),\n",
    "        ('ley', 18892, 1989, 'decreto supremo', 430, 1991), \n",
    "        ('decreto supremo', 938, 2011, 'decreto supremo', 38, 2011), \n",
    "        ('ley', 34, 2020, 'decreto supremo', 160, 2008),\n",
    "        ('decreto supremo', 34, 2020, 'decreto supremo', 160, 2008),\n",
    "        ('decreto con fuerza de ley', 34, 2020, 'decreto supremo', 160, 2008),\n",
    "    ]\n",
    "    \n",
    "    print(f\"Aplicando {len(correcciones)} correcciones...\")\n",
    "    print(\"Normalizando datos del DataFrame...\")\n",
    "    \n",
    "    df_normalizado = df.copy()\n",
    "    df_normalizado['fuente_norm'] = df['fuente'].apply(normalizar_fuente)\n",
    "    df_normalizado['numero_norm'] = df['numero_fuente_ajustado'].apply(normalizar_numero)\n",
    "    df_normalizado['año_norm'] = df['año_fuente'].apply(normalizar_año)\n",
    "    registros_modificados = 0\n",
    "    for correccion in correcciones:\n",
    "        fuente_orig, numero_orig, año_orig, fuente_dest, numero_dest, año_dest = correccion\n",
    "        fuente_orig_norm = normalizar_fuente(fuente_orig)\n",
    "        numero_orig_norm = normalizar_numero(numero_orig)\n",
    "        año_orig_norm = normalizar_año(año_orig)\n",
    "        \n",
    "\n",
    "        condicion = (\n",
    "            (df_normalizado['fuente_norm'] == fuente_orig_norm) & \n",
    "            (df_normalizado['numero_norm'] == numero_orig_norm) & \n",
    "            (df_normalizado['año_norm'] == año_orig_norm)\n",
    "        )\n",
    "        registros_coincidentes = condicion.sum()\n",
    "        \n",
    "        if registros_coincidentes > 0:\n",
    "            df.loc[condicion, 'fuente'] = fuente_dest\n",
    "            df.loc[condicion, 'numero_fuente_ajustado'] = numero_dest\n",
    "            df.loc[condicion, 'año_fuente'] = año_dest\n",
    "            \n",
    "            registros_modificados += registros_coincidentes\n",
    "            print(f\"✓ Corregidos {registros_coincidentes} registros: \"\n",
    "                  f\"{fuente_orig} {numero_orig}/{año_orig} → \"\n",
    "                  f\"{fuente_dest} {numero_dest}/{año_dest}\")\n",
    "        else:\n",
    "            print(f\"⚠ No se encontraron registros para: \"\n",
    "                  f\"{fuente_orig} {numero_orig}/{año_orig}\")\n",
    "    \n",
    "    print(f\"\\nResumen: Se modificaron {registros_modificados} registros en total.\")\n",
    "    return df\n",
    "\n",
    "df = aplicar_correcciones_normativas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a916f-93c2-403c-98ea-1d37d74139ad",
   "metadata": {},
   "source": [
    "### Corrección Automática a Números Altos (que dificilmente serían una coincidencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8c781-ee2a-45cf-a745-7af50156b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_correcciones_por_numero(df):\n",
    "    \"\"\"\n",
    "    Aplica correcciones basándose ÚNICAMENTE en el numero_fuente_ajustado\n",
    "    Si encuentra el número, reemplaza toda la información con los datos correctos\n",
    "    \"\"\"\n",
    "    \n",
    "    def normalizar_numero(numero):\n",
    "        \"\"\"Normaliza el número para comparaciones consistentes\"\"\"\n",
    "        if pd.isna(numero):\n",
    "            return None\n",
    "        try:\n",
    "            return int(float(str(numero).strip()))\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    correcciones_por_numero = {\n",
    "        160: ('decreto supremo', 2008),\n",
    "        193: ('decreto supremo', 1998),\n",
    "        194: ('decreto supremo', 1973),\n",
    "        200: ('decreto supremo', 1993),\n",
    "        211: ('decreto supremo', 1991),\n",
    "        232: ('resolucion', 2002),\n",
    "        236: ('decreto supremo', 1926),\n",
    "        244: ('decreto supremo', 2005),\n",
    "        248: ('decreto supremo', 2007),\n",
    "        259: ('decreto supremo', 1980),\n",
    "        276: ('decreto supremo', 1980),\n",
    "        279: ('decreto supremo', 1983),\n",
    "        291: ('decreto supremo', 2007),\n",
    "        298: ('decreto supremo', 1994),\n",
    "        300: ('decreto supremo', 1994),\n",
    "        327: ('decreto supremo', 1997),\n",
    "        400: ('decreto supremo', 1977),\n",
    "        405: ('decreto supremo', 1983),\n",
    "        430: ('decreto supremo', 1991),\n",
    "        458: ('decreto con fuerza de ley', 1975),\n",
    "        461: ('decreto supremo', 1995),\n",
    "        484: ('decreto supremo', 1990),\n",
    "        531: ('decreto supremo', 1967),\n",
    "        594: ('decreto supremo', 1999),\n",
    "        655: ('decreto supremo', 1940),\n",
    "        725: ('decreto con fuerza de ley', 1967),\n",
    "        735: ('decreto supremo', 1969),\n",
    "        830: ('decreto ley', 1974),\n",
    "        850: ('decreto con fuerza de ley', 1997),\n",
    "        867: ('decreto supremo', 1978),\n",
    "        1122: ('decreto con fuerza de ley', 1981),\n",
    "        1164: ('decreto supremo', 1974),\n",
    "        1215: ('ley', 1978),\n",
    "        1261: ('decreto supremo', 1957),\n",
    "        499: ('resolucion', 2006),\n",
    "        1665: ('decreto supremo', 2002),\n",
    "        2565: ('decreto ley', 1979),\n",
    "        3557: ('decreto ley', 1980),\n",
    "        4188: ('decreto supremo', 1955),\n",
    "        4363: ('decreto supremo', 1931),\n",
    "        4601: ('decreto supremo', 1929),\n",
    "        16744: ('ley', 1968),\n",
    "        17288: ('ley', 1970),\n",
    "        18248: ('ley', 1983),\n",
    "        18378: ('ley', 1984),\n",
    "        18695: ('ley', 1988),\n",
    "        18755: ('ley', 1989),\n",
    "        18834: ('ley', 1989),\n",
    "        19253: ('ley', 1993),\n",
    "        19300: ('ley', 1994),\n",
    "        19473: ('ley', 1996),\n",
    "        19880: ('ley', 2003),\n",
    "        20001: ('ley', 2005),\n",
    "        20096: ('ley', 2006),\n",
    "        20283: ('ley', 2008),\n",
    "        20380: ('ley', 2009),\n",
    "        20389: ('ley', 2009),\n",
    "        20417: ('ley', 2010),\n",
    "        20551: ('ley', 2011),\n",
    "        20879: ('ley', 2015),\n",
    "        20920: ('ley', 2016),\n",
    "        20936: ('ley', 2017),\n",
    "        21455: ('ley', 2022),\n",
    "        1139: ('resolucion', 2013),\n",
    "        1518: ('resolucion', 2013),\n",
    "        172: ('decreto supremo', 1988),\n",
    "        223: ('resolucion', 2015),\n",
    "        359: ('resolucion', 2005),\n",
    "        446: ('decreto supremo', 2006),\n",
    "        610: ('resolucion', 1982),\n",
    "        878: ('decreto supremo', 2011)\n",
    "    }\n",
    "    \n",
    "    print(f\"Aplicando correcciones por número para {len(correcciones_por_numero)} números diferentes...\")\n",
    "    \n",
    "    df['numero_norm_temp'] = df['numero_fuente_ajustado'].apply(normalizar_numero)\n",
    "    \n",
    "    registros_modificados = 0\n",
    "    numeros_encontrados = []\n",
    "    \n",
    "    for numero, (fuente_correcta, año_correcto) in correcciones_por_numero.items():\n",
    "        condicion = df['numero_norm_temp'] == numero\n",
    "        registros_coincidentes = condicion.sum()\n",
    "        \n",
    "        if registros_coincidentes > 0:\n",
    "            registros_actuales = df[condicion][['fuente', 'numero_fuente_ajustado', 'año_fuente']].drop_duplicates()\n",
    "            \n",
    "            print(f\"✓ Número {numero} encontrado en {registros_coincidentes} registros:\")\n",
    "            for _, registro in registros_actuales.iterrows():\n",
    "                print(f\"  {registro['fuente']} {registro['numero_fuente_ajustado']}/{registro['año_fuente']} → {fuente_correcta} {numero}/{año_correcto}\")\n",
    "            df.loc[condicion, 'fuente'] = fuente_correcta\n",
    "            df.loc[condicion, 'numero_fuente_ajustado'] = numero\n",
    "            df.loc[condicion, 'año_fuente'] = año_correcto\n",
    "            \n",
    "            registros_modificados += registros_coincidentes\n",
    "            numeros_encontrados.append(numero)\n",
    "    \n",
    "    df.drop('numero_norm_temp', axis=1, inplace=True)\n",
    "    \n",
    "    numeros_no_encontrados = set(correcciones_por_numero.keys()) - set(numeros_encontrados)\n",
    "    \n",
    "    print(f\"\\nResumen:\")\n",
    "    print(f\"- Se modificaron {registros_modificados} registros en total\")\n",
    "    print(f\"- Se encontraron {len(numeros_encontrados)} números diferentes en los datos\")\n",
    "    \n",
    "    if numeros_no_encontrados:\n",
    "        print(f\"- Números no encontrados en los datos: {sorted(list(numeros_no_encontrados))[:10]}{'...' if len(numeros_no_encontrados) > 10 else ''}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = aplicar_correcciones_por_numero(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6d3fb-0a5a-4daf-a198-48418f1e1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"RUTA\\obligaciones_combinadas_expandidas_corregidas.xlsx\"\n",
    "df.to_excel(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fcdff-6eb8-4b20-9d22-95baf1d496a9",
   "metadata": {},
   "source": [
    "#### Comprimir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec906d7f-2341-42fa-8103-88c5bb453a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprimir_fuentes(df_expandido):\n",
    "    \"\"\"Comprime el DataFrame agrupando por 'obligacion_id'.\"\"\"\n",
    "    columnas_a_agregar = {\n",
    "        col: 'first' for col in df_expandido.columns\n",
    "        if col not in ['obligacion_id', 'fuente', 'numero_fuente_ajustado', 'año_fuente']\n",
    "    }\n",
    "    columnas_a_agregar['fuente'] = lambda x: list(x)\n",
    "    columnas_a_agregar['numero_fuente_ajustado'] = lambda x: list(x)\n",
    "    columnas_a_agregar['año_fuente'] = lambda x: list(x)\n",
    "    df_comprimido = df_expandido.groupby('obligacion_id').agg(columnas_a_agregar).reset_index()\n",
    "    return df_comprimido\n",
    "\n",
    "df['fuente'] = df['fuente'].str.replace('Compromiso ambiental voluntario', 'compromiso ambiental voluntario')\n",
    "df_comprimido = comprimir_fuentes(df)\n",
    "def extraer_pas_y_limpiar(row):\n",
    "    \"\"\"\n",
    "    Busca la fuente 'Permisos Ambientales Sectoriales', extrae su número a una nueva\n",
    "    columna 'pas' y devuelve las listas limpias sin esa fuente.\n",
    "    \"\"\"\n",
    "    fuentes = row['fuente']\n",
    "    numeros = row['numero_fuente_ajustado']\n",
    "    años = row['año_fuente']\n",
    "    \n",
    "    pas_valor = np.nan\n",
    "    fuentes_limpias = []\n",
    "    numeros_limpios = []\n",
    "    años_limpios = []\n",
    "    for i, fuente in enumerate(fuentes):\n",
    "        if fuente == 'permisos ambientales sectoriales':\n",
    "            pas_valor = numeros[i] \n",
    "        else:\n",
    "            fuentes_limpias.append(fuentes[i])\n",
    "            numeros_limpios.append(numeros[i])\n",
    "            años_limpios.append(años[i])\n",
    "            \n",
    "    return pas_valor, fuentes_limpias, numeros_limpios, años_limpios\n",
    "    \n",
    "# Asignamos los resultados a nuevas columnas en el DataFrame\n",
    "df_comprimido[['pas', 'fuente', 'numero_fuente_ajustado', 'año_fuente']] = df_comprimido.apply(extraer_pas_y_limpiar, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a1aa2-1f91-46ed-a63c-792f75aff2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fuentes = df_comprimido['fuente'].str.len().max()\n",
    "for i in range(max_fuentes):\n",
    "    fuente_i = df_comprimido['fuente'].str[i]\n",
    "    numero_i = df_comprimido['numero_fuente_ajustado'].str[i]\n",
    "    año_i = df_comprimido['año_fuente'].str[i]\n",
    "\n",
    "    numeros_validos = pd.to_numeric(numero_i, errors='coerce')\n",
    "    años_validos = pd.to_numeric(año_i, errors='coerce')\n",
    "\n",
    "    texto_fuente = fuente_i.fillna('')\n",
    "    texto_numero = numeros_validos.dropna().astype(int).astype(str)\n",
    "    texto_año = años_validos.dropna().astype(int).astype(str)\n",
    "\n",
    "    texto_base = texto_fuente.add(' ' + texto_numero, fill_value='').str.strip()\n",
    "    columna_final = texto_base.add('/' + texto_año, fill_value='').str.strip()\n",
    "\n",
    "    df_comprimido[f'fuente_{i+1}'] = columna_final.replace('', np.nan)\n",
    "\n",
    "columnas_a_borrar = ['numero_fuente_ajustado', 'año_fuente', 'fuente_5', 'fuente_6', 'fuente_7', 'fuente_8', 'fuente_9', 'fuente_10']\n",
    "df_comprimido = df_comprimido.drop(columns=columnas_a_borrar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019709fd-8890-4141-8ead-eb1abe343449",
   "metadata": {},
   "outputs": [],
   "source": [
    "condicion = (df_comprimido['seccion'] == 'pas') & \\\n",
    "            (df_comprimido['fuente_1'].notna()) & \\\n",
    "            (df_comprimido['pas'].isna()) & \\\n",
    "            (df_comprimido['numero_fuente'].notna())\n",
    "\n",
    "df_comprimido.loc[condicion, 'pas'] = df_comprimido.loc[condicion, 'numero_fuente']\n",
    "df_comprimido.loc[condicion, 'fuente_1'] = np.nan\n",
    "df_comprimido.loc[condicion, 'fuente_2'] = np.nan\n",
    "df_comprimido.loc[condicion, 'fuente_3'] = np.nan\n",
    "df_comprimido.loc[condicion, 'fuente_4'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c61ad-23ee-4436-8f17-0d2c46530510",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"RUTA\\obligaciones_combinadas_corregidas.xlsx\"\n",
    "df_comprimido.to_excel(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4898f-7ceb-40d0-870f-b57b176bf3f8",
   "metadata": {},
   "source": [
    "### GRAFOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6190da-8e7f-4afc-8fee-453e412a7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_excel = r\"RUTA\\normativa.xlsx\"\n",
    "font_path = r\"RUTA\\fuente\\gobCL_Bold.ttf\"\n",
    "try:\n",
    "    font_manager.fontManager.addfont(font_path)\n",
    "    plt.rcParams['font.family'] = 'gobCL'\n",
    "    print(\"Fuente gobCL cargada exitosamente\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo cargar la fuente gobCL: {e}\")\n",
    "    print(\"Usando fuente por defecto\")\n",
    "\n",
    "try:\n",
    "    pareadas = pd.read_excel(archivo_excel, sheet_name=\"Pareadas\")\n",
    "    print(\"Datos de 'Pareadas' cargados:\")\n",
    "    print(pareadas.describe())\n",
    "    ponderacion = pd.read_excel(archivo_excel, sheet_name=\"Ponderación\")\n",
    "    print(\"\\nDatos de 'Ponderación' cargados:\")\n",
    "    print(ponderacion.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo en la ruta especificada:\\n{archivo_excel}\")\n",
    "    exit()\n",
    "\n",
    "edge_list = pareadas.rename(columns={ 'fuente_1': 'P1', 'fuente_2': 'P2','Valor': 'weight'}).copy()\n",
    "relaciones_reales = edge_list[edge_list['P1'] != edge_list['P2']]\n",
    "\n",
    "print(f\"\\nAnálisis de datos:\")\n",
    "print(f\"Total de filas: {len(edge_list)}\")\n",
    "print(f\"Auto-loops (fuente consigo misma): {len(edge_list) - len(relaciones_reales)}\")\n",
    "print(f\"Relaciones entre diferentes fuentes: {len(relaciones_reales)}\")\n",
    "\n",
    "if not relaciones_reales.empty:\n",
    "    print(\"¡Se encontraron relaciones reales entre fuentes! Usando estos datos.\")\n",
    "    edge_list = relaciones_reales\n",
    "else:\n",
    "    print(\"Advertencia: Solo hay auto-loops. No se pueden construir conexiones significativas.\")\n",
    "    edge_list = pd.DataFrame(columns=['P1', 'P2', 'weight'])\n",
    "\n",
    "if not edge_list.empty:\n",
    "    min_w = edge_list['weight'].min()\n",
    "    max_w = edge_list['weight'].max()\n",
    "\n",
    "    if max_w > min_w:\n",
    "        edge_list['weight_normalized'] = (edge_list['weight'] - min_w) / (max_w - min_w)\n",
    "    else:\n",
    "        edge_list['weight_normalized'] = 0.5\n",
    "\n",
    "    edge_list['layout_weight'] = (edge_list['weight_normalized'] * 9) + 1  \n",
    "    edge_list[\"inv_weight\"] = 1.01 - edge_list['weight_normalized']\n",
    "    min_width, max_width = 1, 8\n",
    "    edge_list['line_width'] = min_width + (edge_list['weight_normalized'] * (max_width - min_width))\n",
    "    \n",
    "    print(\"\\nEdge list con atributos de visualización:\")\n",
    "    print(edge_list[['P1', 'P2', 'weight_normalized', 'layout_weight', 'line_width']].head())\n",
    "else:\n",
    "    print(\"\\nNo hay aristas válidas para construir la red.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "\n",
    "g = nx.from_pandas_edgelist(edge_list, source=\"P1\", target=\"P2\", \n",
    "                           edge_attr=[\"weight_normalized\", \"inv_weight\", \"layout_weight\", \"line_width\"])\n",
    "print(f\"\\nRed completa - Nodos: {g.number_of_nodes()}, Aristas: {g.number_of_edges()}\")\n",
    "\n",
    "\n",
    "mst = nx.minimum_spanning_tree(g, weight='inv_weight')\n",
    "print(f\"MST - Nodos: {mst.number_of_nodes()}, Aristas: {mst.number_of_edges()}\")\n",
    "ps = nx.Graph()\n",
    "\n",
    "# 1. Añadir todas las aristas del MST\n",
    "for u, v, data in mst.edges(data=True):\n",
    "    ps.add_edge(u, v, \n",
    "                weight_normalized=data.get(\"weight_normalized\", 0.5),\n",
    "                layout_weight=data.get(\"layout_weight\", 5),\n",
    "                line_width=data.get(\"line_width\", 3))\n",
    "\n",
    "# 2. Añadir aristas adicionales que superen un umbral de similitud\n",
    "umbral = 0.017544 ## Percentil 25\n",
    "for u, v, data in g.edges(data=True):\n",
    "    if data['weight_normalized'] >= umbral:\n",
    "        ps.add_edge(u, v, \n",
    "                    weight_normalized=data.get(\"weight_normalized\", 0.5),\n",
    "                    layout_weight=data.get(\"layout_weight\", 5),\n",
    "                    line_width=data.get(\"line_width\", 3))\n",
    "\n",
    "print(f\"\\nRed final (ps) - Nodos: {ps.number_of_nodes()}, Aristas: {ps.number_of_edges()}\")\n",
    "\n",
    "node_list = list(ps.nodes)\n",
    "edge_list = list(ps.edges)\n",
    "positions = nx.spring_layout(ps)\n",
    "print(positions)\n",
    "\n",
    "fig = plt.figure(figsize=(50,20))\n",
    "ax = fig.gca()\n",
    "nx.draw_networkx(ps, ax=ax, node_size=750, width=1)\n",
    "\n",
    "plt.savefig(\"productspace1.png\")\n",
    "\n",
    "ponderacion.drop_duplicates(subset=['fuente'], keep='first', inplace=True)\n",
    "ponderacion_dict = ponderacion.set_index('fuente').to_dict('index')\n",
    "nodos_a_eliminar = []\n",
    "for node_id in ps.nodes():\n",
    "    if node_id in ponderacion_dict:\n",
    "        ministerio = ponderacion_dict[node_id].get('ministerio', 'Sin clasificar')\n",
    "        if ministerio == 'Sin clasificar':\n",
    "            nodos_a_eliminar.append(node_id)\n",
    "    else:\n",
    "        nodos_a_eliminar.append(node_id)\n",
    "\n",
    "# Eliminar nodos \"Sin clasificar\"\n",
    "ps.remove_nodes_from(nodos_a_eliminar)\n",
    "print(f\"Eliminados {len(nodos_a_eliminar)} nodos 'Sin clasificar'\")\n",
    "print(f\"Red final después de filtrado - Nodos: {ps.number_of_nodes()}, Aristas: {ps.number_of_edges()}\")\n",
    "for node_id in ps.nodes():\n",
    "    if node_id in ponderacion_dict:\n",
    "        nx.set_node_attributes(ps, {node_id: ponderacion_dict[node_id]})\n",
    "\n",
    "ultra_orange_colors = [\n",
    "    \"#838D9B\", \"#D2610C\", \"#F4D25A\", \"#EB8A2D\", \"#A03B26\", \"#9c6111\", \n",
    "    \"#bab106\", \"#FB8281\", \"#C42424\", \"#2b478a\", \"#6A9FB0\", \"#7B4F71\",\n",
    "]\n",
    "\n",
    "ministerios_unicos = ponderacion['ministerio'].unique()\n",
    "color_map_ministerios = {\n",
    "    ministerio: ultra_orange_colors[i % len(ultra_orange_colors)]\n",
    "    for i, ministerio in enumerate(ministerios_unicos)\n",
    "}\n",
    "\n",
    "# Calcular rangos para el tamaño de nodos\n",
    "ponderaciones = [data.get('ponderación', 1) for node, data in ps.nodes(data=True)]\n",
    "min_ponderacion = min(ponderaciones) if ponderaciones else 1\n",
    "max_ponderacion = max(ponderaciones) if ponderaciones else 1\n",
    "\n",
    "\n",
    "if ps.number_of_nodes() > 0:\n",
    "    plt.figure(figsize=(22, 18))\n",
    "\n",
    "    pos = nx.spring_layout(ps, \n",
    "                          weight='layout_weight', \n",
    "                          k=1.0/np.sqrt(ps.number_of_nodes()),  \n",
    "                          iterations=2000,  \n",
    "                          seed=42)\n",
    "\n",
    "    # Preparar colores de nodos\n",
    "    node_colors = [color_map_ministerios.get(ps.nodes[node].get('ministerio', 'Sin clasificar')) \n",
    "                   for node in ps.nodes()]\n",
    "    node_sizes = []\n",
    "    for node in ps.nodes():\n",
    "        pond = ps.nodes[node].get('ponderación', min_ponderacion)\n",
    "        if max_ponderacion > min_ponderacion:\n",
    "            normalized = (pond - min_ponderacion) / (max_ponderacion - min_ponderacion)\n",
    "            size = 300 + normalized * 5500\n",
    "        else:\n",
    "            size = 500\n",
    "        node_sizes.append(size)\n",
    "\n",
    "    edge_widths = [data['line_width'] for u, v, data in ps.edges(data=True)]\n",
    "    edge_colors = []\n",
    "    for u, v, data in ps.edges(data=True):\n",
    "        intensity = data['weight_normalized']\n",
    "        color_intensity = 0.7 \n",
    "        edge_colors.append((color_intensity, color_intensity, color_intensity))\n",
    "\n",
    "    nx.draw_networkx_edges(ps, pos, \n",
    "                          width=edge_widths,\n",
    "                          edge_color=edge_colors,\n",
    "                          alpha=0.8)\n",
    "\n",
    "\n",
    "    node_data = [(node, size) for node, size in zip(ps.nodes(), node_sizes)]\n",
    "    node_data.sort(key=lambda x: x[1])  \n",
    "    \n",
    "    for node, size in node_data:\n",
    "        node_idx = list(ps.nodes()).index(node)\n",
    "        nx.draw_networkx_nodes(ps, pos, \n",
    "                              nodelist=[node],\n",
    "                              node_color=[node_colors[node_idx]], \n",
    "                              node_size=[size],\n",
    "                              alpha=1.0,\n",
    "                              linewidths=0.7, \n",
    "                              edgecolors='#2C2C2C')  \n",
    "\n",
    "    def texts_overlap(text1, text2, threshold=0.1):\n",
    "        \"\"\"Detecta si dos textos se solapan significativamente\"\"\"\n",
    "        bbox1 = text1.get_window_extent()\n",
    "        bbox2 = text2.get_window_extent()\n",
    "        \n",
    "        # Convertir a coordenadas de datos\n",
    "        bbox1_data = bbox1.transformed(plt.gca().transData.inverted())\n",
    "        bbox2_data = bbox2.transformed(plt.gca().transData.inverted())\n",
    "        \n",
    "        x1_min, y1_min = bbox1_data.x0, bbox1_data.y0\n",
    "        x1_max, y1_max = bbox1_data.x1, bbox1_data.y1\n",
    "        x2_min, y2_min = bbox2_data.x0, bbox2_data.y0\n",
    "        x2_max, y2_max = bbox2_data.x1, bbox2_data.y1\n",
    "        \n",
    "        if (x1_max > x2_min and x1_min < x2_max and \n",
    "            y1_max > y2_min and y1_min < y2_max):\n",
    "            \n",
    "            intersect_width = min(x1_max, x2_max) - max(x1_min, x2_min)\n",
    "            intersect_height = min(y1_max, y2_max) - max(y1_min, y2_min)\n",
    "            intersect_area = intersect_width * intersect_height\n",
    "            \n",
    "            area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "            area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "            min_area = min(area1, area2)\n",
    "            \n",
    "            return intersect_area / min_area > threshold\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "    # Crear todas las etiquetas centradas inicialmente\n",
    "    texts = []\n",
    "    for i, node in enumerate(ps.nodes()):\n",
    "        # Reemplazar espacios por saltos de línea\n",
    "        label_multilinea = str(node).replace(' ', '\\n')\n",
    "        x, y = pos[node]\n",
    "        \n",
    "        node_size = node_sizes[i]\n",
    "        min_font_size, max_font_size = 11, 20\n",
    "        min_node_size, max_node_size = min(node_sizes), max(node_sizes)\n",
    "        \n",
    "        if max_node_size > min_node_size:\n",
    "            normalized_size = (node_size - min_node_size) / (max_node_size - min_node_size)\n",
    "            font_size = min_font_size + (normalized_size * (max_font_size - min_font_size))\n",
    "        else:\n",
    "            font_size = 12\n",
    "        \n",
    "        text = plt.text(x, y, label_multilinea, \n",
    "                       fontsize=font_size, \n",
    "                       fontweight='bold', \n",
    "                       color='#1C1C1C',\n",
    "                       family='gobCL',\n",
    "                       ha='center', \n",
    "                       va='center')\n",
    "        texts.append(text)\n",
    "\n",
    "    # Forzar renderizado para obtener dimensiones correctas\n",
    "    plt.gcf().canvas.draw()\n",
    "\n",
    "    overlapping_texts = []\n",
    "    for i, text1 in enumerate(texts):\n",
    "        for j, text2 in enumerate(texts[i+1:], i+1):\n",
    "            if texts_overlap(text1, text2, threshold=0.2):  \n",
    "                if text1 not in overlapping_texts:\n",
    "                    overlapping_texts.append(text1)\n",
    "                if text2 not in overlapping_texts:\n",
    "                    overlapping_texts.append(text2)\n",
    "\n",
    "    print(f\"Etiquetas con solapamiento detectado: {len(overlapping_texts)} de {len(texts)}\")\n",
    "\n",
    "    \n",
    "    # Ordenar ministerios para que \"Otros\" aparezca al final\n",
    "    ministerios_ordenados = sorted(color_map_ministerios.items(), \n",
    "                                  key=lambda x: (x[0] == 'Otros', x[0]))\n",
    "    \n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=color, markersize=16, label=ministerio,\n",
    "                             markeredgecolor='#1C1C1C', markeredgewidth=1.5)\n",
    "                      for ministerio, color in ministerios_ordenados]\n",
    "    \n",
    "    legend = plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1.02, 0.5),\n",
    "                       frameon=True, fancybox=True, shadow=True, \n",
    "                       fontsize=16, title='Ministerio', title_fontsize=18)\n",
    "    \n",
    "    # Configurar estilo de la leyenda\n",
    "    legend.get_frame().set_facecolor('#FFFFFF')\n",
    "    legend.get_frame().set_edgecolor('#1C1C1C')\n",
    "    legend.get_frame().set_linewidth(1.5)\n",
    "    \n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontfamily('gobCL')\n",
    "        text.set_color('#1C1C1C')\n",
    "    \n",
    "    legend.get_title().set_fontfamily('gobCL')\n",
    "    legend.get_title().set_color('#1C1C1C')\n",
    "    legend.get_title().set_fontweight('bold')\n",
    "    \n",
    "    plt.gcf().patch.set_facecolor('#FFFFFF')\n",
    "    plt.gca().set_facecolor('#FFFFFF')\n",
    "    \n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.subplots_adjust(left=0.02, right=0.85, top=0.95, bottom=0.02)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"red_fuentes_normativas_ultra_orange.pdf\", dpi=300, bbox_inches='tight', \n",
    "               facecolor='#FFFFFF', edgecolor='none', pad_inches=0.1)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n¡Gráfico con estilo Ultra Orange guardado como 'red_fuentes_normativas_ultra_orange.pdf'!\")\n",
    "    \n",
    "    weights = [data['weight_normalized'] for u, v, data in ps.edges(data=True)]\n",
    "    widths = [data['line_width'] for u, v, data in ps.edges(data=True)]\n",
    "    \n",
    "    print(f\"\\n=== ESTADÍSTICAS DE ARISTAS ===\")\n",
    "    print(f\"Peso promedio (normalizado): {np.mean(weights):.3f}\")\n",
    "    print(f\"Peso máximo: {np.max(weights):.3f}\")\n",
    "    print(f\"Peso mínimo: {np.min(weights):.3f}\")\n",
    "    print(f\"Ancho de línea promedio: {np.mean(widths):.1f}\")\n",
    "    print(f\"Ancho máximo: {np.max(widths):.1f}\")\n",
    "    print(f\"Ancho mínimo: {np.min(widths):.1f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo se puede crear la visualización: la red no tiene nodos.\")\n",
    "\n",
    "print(f\"\\n=== ESTADÍSTICAS DE LA RED ===\")\n",
    "if ps.number_of_nodes() > 0:\n",
    "    print(f\"Número de nodos: {ps.number_of_nodes()}\")\n",
    "    print(f\"Número de aristas: {ps.number_of_edges()}\")\n",
    "    print(f\"Densidad de la red: {nx.density(ps):.4f}\")\n",
    "\n",
    "    if nx.is_connected(ps):\n",
    "        print(\"La red está completamente conectada.\")\n",
    "    else:\n",
    "        print(f\"La red no está conectada. Componentes: {nx.number_connected_components(ps)}\")\n",
    "\n",
    "    centralidad = nx.degree_centrality(ps)\n",
    "    top_centrales = sorted(centralidad.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "    print(f\"\\n--- TOP 5 FUENTES MÁS CENTRALES ---\")\n",
    "    for fuente, cent in top_centrales:\n",
    "        ministerio = ps.nodes[fuente].get('ministerio', 'N/A')\n",
    "        print(f\"- {fuente} (Ministerio: {ministerio}, Centralidad: {cent:.3f})\")\n",
    "        \n",
    "    edge_weights = [(u, v, data['weight_normalized']) for u, v, data in ps.edges(data=True)]\n",
    "    top_edges = sorted(edge_weights, key=lambda x: x[2], reverse=True)[:5]\n",
    "    \n",
    "    print(f\"\\n--- TOP 5 CONEXIONES MÁS FUERTES ---\")\n",
    "    for u, v, weight in top_edges:\n",
    "        print(f\"- {u} ↔ {v} (Similitud: {weight:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcb19f-4aa6-4908-895c-6eb1e13cb86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== EXPORTANDO ARISTAS A EXCEL ===\")\n",
    "\n",
    "if ps.number_of_nodes() > 0 and ps.number_of_edges() > 0:\n",
    "    # Crear lista para almacenar las aristas\n",
    "    aristas_finales = []\n",
    "    \n",
    "    # Extraer todas las aristas de la red final con sus atributos\n",
    "    for u, v, data in ps.edges(data=True):\n",
    "        valor_original = None\n",
    "        for _, row in pareadas.iterrows():\n",
    "            if ((row['fuente_1'] == u and row['fuente_2'] == v) or \n",
    "                (row['fuente_1'] == v and row['fuente_2'] == u)):\n",
    "                valor_original = row['Valor']\n",
    "                break\n",
    "        \n",
    "        arista = {\n",
    "            'fuente_1': u,\n",
    "            'fuente_2': v,\n",
    "            'ponderacion': valor_original if valor_original is not None else 0,  \n",
    "            'peso_normalizado': data.get('weight_normalized', 0),  \n",
    "            'ancho_linea': data.get('line_width', 1), \n",
    "            'peso_layout': data.get('layout_weight', 1)  \n",
    "        }\n",
    "        aristas_finales.append(arista)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df_aristas = pd.DataFrame(aristas_finales)\n",
    "    df_aristas = df_aristas.sort_values('ponderacion', ascending=False)\n",
    "    df_aristas['ministerio_fuente_1'] = df_aristas['fuente_1'].map(\n",
    "        lambda x: ps.nodes[x].get('ministerio', 'N/A') if x in ps.nodes else 'N/A'\n",
    "    )\n",
    "    df_aristas['ministerio_fuente_2'] = df_aristas['fuente_2'].map(\n",
    "        lambda x: ps.nodes[x].get('ministerio', 'N/A') if x in ps.nodes else 'N/A'\n",
    "    )\n",
    "    df_aristas['ponderacion_nodo_1'] = df_aristas['fuente_1'].map(\n",
    "        lambda x: ponderacion_dict[x].get('ponderación', 0) if x in ponderacion_dict else 0\n",
    "    )\n",
    "    df_aristas['ponderacion_nodo_2'] = df_aristas['fuente_2'].map(\n",
    "        lambda x: ponderacion_dict[x].get('ponderación', 0) if x in ponderacion_dict else 0\n",
    "    )\n",
    "    directorio_salida = os.path.dirname(archivo_excel)\n",
    "    archivo_salida = os.path.join(directorio_salida, \"aristas_red_final.xlsx\")\n",
    "    try:\n",
    "        with pd.ExcelWriter(archivo_salida, engine='openpyxl') as writer:\n",
    "            df_aristas.to_excel(writer, sheet_name='Aristas_Red_Final', index=False)\n",
    "            estadisticas = {\n",
    "                'Métrica': [\n",
    "                    'Número total de aristas',\n",
    "                    'Número de nodos conectados',\n",
    "                    'Ponderación promedio',\n",
    "                    'Ponderación máxima',\n",
    "                    'Ponderación mínima',\n",
    "                    'Densidad de la red',\n",
    "                    'Red conectada'\n",
    "                ],\n",
    "                'Valor': [\n",
    "                    ps.number_of_edges(),\n",
    "                    ps.number_of_nodes(),\n",
    "                    df_aristas['ponderacion'].mean(),\n",
    "                    df_aristas['ponderacion'].max(),\n",
    "                    df_aristas['ponderacion'].min(),\n",
    "                    nx.density(ps),\n",
    "                    'Sí' if nx.is_connected(ps) else 'No'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            df_estadisticas = pd.DataFrame(estadisticas)\n",
    "            df_estadisticas.to_excel(writer, sheet_name='Estadisticas', index=False)\n",
    "            \n",
    "            # Hoja con resumen por ministerio\n",
    "            ministerios_conectados = set()\n",
    "            for _, row in df_aristas.iterrows():\n",
    "                ministerios_conectados.add(row['ministerio_fuente_1'])\n",
    "                ministerios_conectados.add(row['ministerio_fuente_2'])\n",
    "            \n",
    "            resumen_ministerios = []\n",
    "            for ministerio in ministerios_conectados:\n",
    "                if ministerio != 'N/A':\n",
    "                    # Contar conexiones donde participa este ministerio\n",
    "                    conexiones = len(df_aristas[\n",
    "                        (df_aristas['ministerio_fuente_1'] == ministerio) | \n",
    "                        (df_aristas['ministerio_fuente_2'] == ministerio)\n",
    "                    ])\n",
    "                    \n",
    "                    # Promedio de ponderación\n",
    "                    ponderaciones_ministerio = df_aristas[\n",
    "                        (df_aristas['ministerio_fuente_1'] == ministerio) | \n",
    "                        (df_aristas['ministerio_fuente_2'] == ministerio)\n",
    "                    ]['ponderacion']\n",
    "                    \n",
    "                    resumen_ministerios.append({\n",
    "                        'Ministerio': ministerio,\n",
    "                        'Número_Conexiones': conexiones,\n",
    "                        'Ponderación_Promedio': ponderaciones_ministerio.mean(),\n",
    "                        'Ponderación_Máxima': ponderaciones_ministerio.max()\n",
    "                    })\n",
    "            \n",
    "            df_ministerios = pd.DataFrame(resumen_ministerios)\n",
    "            df_ministerios = df_ministerios.sort_values('Número_Conexiones', ascending=False)\n",
    "            df_ministerios.to_excel(writer, sheet_name='Resumen_Ministerios', index=False)\n",
    "        \n",
    "        print(f\"✅ Archivo Excel creado exitosamente:\")\n",
    "        print(f\"📄 Ruta: {archivo_salida}\")\n",
    "        print(f\"📊 Total de aristas exportadas: {len(df_aristas)}\")\n",
    "        print(f\"📈 Ponderación promedio: {df_aristas['ponderacion'].mean():.4f}\")\n",
    "        \n",
    "        # Mostrar las top 10 conexiones más fuertes\n",
    "        print(f\"\\n--- TOP 10 CONEXIONES MÁS FUERTES (VALORES ORIGINALES) ---\")\n",
    "        top_10 = df_aristas.head(10)\n",
    "        for idx, row in top_10.iterrows():\n",
    "            print(f\"• {row['fuente_1']} ↔ {row['fuente_2']} (Valor original: {row['ponderacion']:.4f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al guardar el archivo Excel: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No se pueden exportar las aristas: la red no tiene conexiones válidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06943b34-70c7-494d-8b5c-dc2d8ed3056e",
   "metadata": {},
   "source": [
    "### Otras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22957d-8913-49d1-a023-e48f95d3c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PASO EXTRA: ANÁLISIS DE EXCLUSIONES ---\n",
    "print(\"\\n=== ANÁLISIS DE ELEMENTOS EXCLUIDOS ===\")\n",
    "\n",
    "# La lista 'nodos_a_eliminar' ya fue creada en el Paso 7.\n",
    "print(f\"\\n--- Nodos Excluidos ---\")\n",
    "print(f\"Total de nodos excluidos (ej. 'Sin clasificar'): {len(nodos_a_eliminar)}\")\n",
    "if len(nodos_a_eliminar) > 0:\n",
    "    print(\"Algunos nodos excluidos:\")\n",
    "    print(nodos_a_eliminar[:10]) # Imprime los primeros 10 para no saturar la consola\n",
    "\n",
    "# Comparamos las aristas del grafo completo 'g' con las del grafo final 'ps'\n",
    "aristas_totales = set(g.edges())\n",
    "aristas_finales = set(ps.edges())\n",
    "\n",
    "aristas_excluidas = aristas_totales - aristas_finales\n",
    "\n",
    "print(f\"\\n--- Aristas Excluidas ---\")\n",
    "print(f\"Aristas en el grafo completo (g): {len(aristas_totales)}\")\n",
    "print(f\"Aristas en el grafo final (ps): {len(aristas_finales)}\")\n",
    "print(f\"Total de aristas excluidas (relaciones débiles y no estructurales): {len(aristas_excluidas)}\")\n",
    "\n",
    "if len(aristas_excluidas) > 0:\n",
    "    # Para ver cuáles fueron, podemos convertirlas a una lista e imprimir algunas\n",
    "    lista_aristas_excluidas = list(aristas_excluidas)\n",
    "    print(\"Algunas aristas excluidas (Fuente 1, Fuente 2):\")\n",
    "    print(lista_aristas_excluidas[:10]) # Imprime las primeras 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07726f7a-caa7-4913-af5c-17488dca1979",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eeecb2-d603-4be7-8465-c5dd9b24bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_excel = r\"RUTA\\normativa.xlsx\"\n",
    "\n",
    "# Cargar hoja \"Pareadas\" (equivalente a edge_list)\n",
    "try:\n",
    "    pareadas = pd.read_excel(archivo_excel, sheet_name=\"Pareadas\")\n",
    "    print(\"Datos de pareadas cargados:\")\n",
    "    print(pareadas.head())\n",
    "\n",
    "    # Cargar hoja \"Ponderación\" (equivalente a prestaciones)\n",
    "    ponderacion = pd.read_excel(archivo_excel, sheet_name=\"Ponderación\")\n",
    "    print(\"\\nDatos de ponderación cargados:\")\n",
    "    print(ponderacion.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se pudo encontrar el archivo en la ruta especificada: {archivo_excel}\")\n",
    "    print(\"Por favor, actualiza la variable 'archivo_excel' con la ruta correcta.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- PASO 2: ANALIZAR ESTRUCTURA DE DATOS ---\n",
    "edge_list = pareadas.rename(columns={\n",
    "    'fuente_1': 'P1',\n",
    "    'fuente_2': 'P2',\n",
    "    'Valor': 'weight'\n",
    "}).copy()\n",
    "\n",
    "# Verificar si hay relaciones reales entre diferentes fuentes\n",
    "auto_loops = edge_list[edge_list['P1'] == edge_list['P2']]\n",
    "relaciones_reales = edge_list[edge_list['P1'] != edge_list['P2']]\n",
    "\n",
    "print(f\"\\nAnálisis de datos:\")\n",
    "print(f\"Total de filas: {len(edge_list)}\")\n",
    "print(f\"Auto-loops (fuente consigo misma): {len(auto_loops)}\")\n",
    "print(f\"Relaciones entre diferentes fuentes: {len(relaciones_reales)}\")\n",
    "\n",
    "if len(relaciones_reales) > 0:\n",
    "    print(\"¡Encontramos relaciones reales entre fuentes!\")\n",
    "    edge_list = relaciones_reales\n",
    "else:\n",
    "    print(\"Solo hay auto-loops. Creando red basada en similitudes por ministerio...\")\n",
    "\n",
    "    # Si solo hay auto-loops, crear conexiones artificiales basadas en ministerio\n",
    "    # Primero, obtener la información de cada fuente\n",
    "    fuentes_con_info = []\n",
    "\n",
    "    for _, row in auto_loops.iterrows():\n",
    "        fuente = row['P1']\n",
    "        peso = row['weight']\n",
    "\n",
    "        # Buscar info en ponderación\n",
    "        info_fuente = ponderacion[ponderacion['fuente'] == fuente]\n",
    "        if len(info_fuente) > 0:\n",
    "            ministerio = info_fuente.iloc[0]['ministerio']\n",
    "            ponderacion_val = info_fuente.iloc[0]['ponderación']\n",
    "        else:\n",
    "            ministerio = 'Sin clasificar'\n",
    "            ponderacion_val = 1\n",
    "\n",
    "        fuentes_con_info.append({\n",
    "            'fuente': fuente,\n",
    "            'peso_original': peso,\n",
    "            'ministerio': ministerio,\n",
    "            'ponderacion': ponderacion_val\n",
    "        })\n",
    "\n",
    "    # Crear DataFrame de fuentes\n",
    "    df_fuentes = pd.DataFrame(fuentes_con_info)\n",
    "\n",
    "    # Crear conexiones entre fuentes del mismo ministerio\n",
    "    new_edge_list = []\n",
    "\n",
    "    for ministerio in df_fuentes['ministerio'].unique():\n",
    "        fuentes_ministerio = df_fuentes[df_fuentes['ministerio'] == ministerio]\n",
    "\n",
    "        # Conectar todas las fuentes del mismo ministerio entre sí\n",
    "        for i, fuente1 in fuentes_ministerio.iterrows():\n",
    "            for j, fuente2 in fuentes_ministerio.iterrows():\n",
    "                if fuente1['fuente'] != fuente2['fuente']: # Evitar auto-loops\n",
    "                    # El peso es proporcional al promedio de sus pesos originales\n",
    "                    peso_conexion = (fuente1['peso_original'] + fuente2['peso_original']) / 2\n",
    "\n",
    "                    new_edge_list.append({\n",
    "                        'P1': fuente1['fuente'],\n",
    "                        'P2': fuente2['fuente'],\n",
    "                        'weight': peso_conexion\n",
    "                    })\n",
    "\n",
    "    # Crear conexiones entre ministerios (con menor peso)\n",
    "    ministerios = df_fuentes['ministerio'].unique()\n",
    "    for i, min1 in enumerate(ministerios):\n",
    "        for j, min2 in enumerate(ministerios):\n",
    "            if i < j: # Evitar duplicados\n",
    "                # Tomar las top fuentes de cada ministerio\n",
    "                fuentes_min1 = df_fuentes[df_fuentes['ministerio'] == min1].nlargest(3, 'peso_original')\n",
    "                fuentes_min2 = df_fuentes[df_fuentes['ministerio'] == min2].nlargest(3, 'peso_original')\n",
    "\n",
    "                # Conectar algunas fuentes entre ministerios\n",
    "                for _, f1 in fuentes_min1.iterrows():\n",
    "                    for _, f2 in fuentes_min2.iterrows():\n",
    "                        peso_inter_ministerial = (f1['peso_original'] + f2['peso_original']) / 4 # Peso menor\n",
    "                        new_edge_list.append({\n",
    "                            'P1': f1['fuente'],\n",
    "                            'P2': f2['fuente'],\n",
    "                            'weight': peso_inter_ministerial\n",
    "                        })\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    edge_list = pd.DataFrame(new_edge_list)\n",
    "\n",
    "    print(f\"Red sintética creada: {len(edge_list)} conexiones\")\n",
    "    print(f\"Conexiones intra-ministeriales y inter-ministeriales generadas\")\n",
    "\n",
    "\n",
    "if len(edge_list) > 0:\n",
    "    min_w = edge_list['weight'].min()\n",
    "    max_w = edge_list['weight'].max()\n",
    "\n",
    "    if max_w != min_w:\n",
    "        edge_list['weight'] = (edge_list['weight'] - min_w) / (max_w - min_w)\n",
    "    else:\n",
    "        edge_list['weight'] = 0.5\n",
    "\n",
    "    # Calcular peso invertido para MST\n",
    "    edge_list[\"inv_weight\"] = 1 - edge_list['weight']\n",
    "\n",
    "    print(\"\\nEdge list normalizado:\")\n",
    "    print(edge_list.head())\n",
    "else:\n",
    "    print(\"No hay aristas válidas después de filtrar auto-loops\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "g = nx.from_pandas_edgelist(edge_list, source=\"P1\", target=\"P2\", edge_attr=[\"weight\", \"inv_weight\"])\n",
    "print(f\"\\n# de Nodos: {g.number_of_nodes()}\")\n",
    "print(f\"# de Aristas: {g.number_of_edges()}\")\n",
    "\n",
    "\n",
    "mst = nx.minimum_spanning_tree(g, weight='inv_weight')\n",
    "print(f\"MST - # de Nodos: {mst.number_of_nodes()}\")\n",
    "print(f\"MST - # de Aristas: {mst.number_of_edges()}\")\n",
    "\n",
    "ps = nx.Graph()\n",
    "\n",
    "# Add MST ('weight' attribute only)\n",
    "for u, v, w in mst.edges(data=True):\n",
    "    ps.add_edge(u, v, **{'weight': w[\"weight\"]})\n",
    "\n",
    "# Add Edges > 0.55\n",
    "umbral = 0.55\n",
    "for u, v, w in g.edges(data=True):\n",
    "    if w['weight'] >= umbral:\n",
    "        ps.add_edge(u, v, **{'weight': w[\"weight\"]})\n",
    "\n",
    "print(f\"\\nRed final - # de Nodos: {ps.number_of_nodes()}\")\n",
    "print(f\"Red final - # de Aristas: {ps.number_of_edges()}\")\n",
    "\n",
    "duplicados = ponderacion['fuente'].duplicated().sum()\n",
    "if duplicados > 0:\n",
    "    print(f\"Eliminando {duplicados} fuentes duplicadas en ponderación...\")\n",
    "    ponderacion = ponderacion.drop_duplicates(subset=['fuente'], keep='first')\n",
    "\n",
    "# Preparar diccionario de ponderación\n",
    "ponderacion_dict = ponderacion.set_index('fuente').to_dict('index')\n",
    "\n",
    "# Asignar atributos a cada nodo\n",
    "for node_id in ps.nodes():\n",
    "    if node_id in ponderacion_dict:\n",
    "        node_attributes = ponderacion_dict[node_id]\n",
    "        nx.set_node_attributes(ps, {node_id: node_attributes})\n",
    "    else:\n",
    "        # Valores por defecto para nodos sin información\n",
    "        nx.set_node_attributes(ps, {node_id: {\n",
    "            'ministerio': 'Sin clasificar',\n",
    "            'ponderación': 1\n",
    "        }})\n",
    "\n",
    "# --- PASO 8: FUNCIONES PARA VISUALIZACIÓN ---\n",
    "# Rangos para escalado\n",
    "ponderaciones = ponderacion['ponderación'].dropna()\n",
    "min_ponderacion = ponderaciones.min() if not ponderaciones.empty else 1\n",
    "max_ponderacion = ponderaciones.max() if not ponderaciones.empty else 1\n",
    "\n",
    "def get_node_scale_factor(node: dict):\n",
    "    \"\"\"Calcula factor de escala basado en ponderación.\"\"\"\n",
    "    ponderacion_valor = node.get('properties', {}).get('ponderación', min_ponderacion)\n",
    "\n",
    "    if max_ponderacion == min_ponderacion:\n",
    "        return 5.0\n",
    "\n",
    "    normalized = (ponderacion_valor - min_ponderacion) / (max_ponderacion - min_ponderacion)\n",
    "    min_scale = 1\n",
    "    max_scale = 5\n",
    "    scale = min_scale + normalized * (max_scale - min_scale)\n",
    "    return scale\n",
    "\n",
    "def get_node_label_style(node: dict):\n",
    "    \"\"\"Retorna estilo para etiqueta del nodo.\"\"\"\n",
    "    fuente_nombre = node.get('id', str(node))\n",
    "    return {\n",
    "        'text': fuente_nombre,\n",
    "        'wrapping': 'word',\n",
    "        'maximumWidth': 120,\n",
    "        'textAlignment': 'center',\n",
    "        'fontSize': 8\n",
    "    }\n",
    "\n",
    "# Mapeo de colores por ministerio\n",
    "ministerios = ponderacion['ministerio'].unique()\n",
    "colores_ministerios = [\n",
    "    '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
    "    '#aec7e8', '#ffbb78', '#98df8a', '#ff9896', '#c5b0d5'\n",
    "]\n",
    "\n",
    "color_map_ministerios = {\n",
    "    ministerio: colores_ministerios[i % len(colores_ministerios)]\n",
    "    for i, ministerio in enumerate(ministerios)\n",
    "}\n",
    "\n",
    "def get_node_color(node):\n",
    "    \"\"\"Obtiene color del nodo basado en ministerio.\"\"\"\n",
    "    ministerio = node.get('properties', {}).get('ministerio', 'Sin clasificar')\n",
    "    return color_map_ministerios.get(ministerio, 'grey')\n",
    "\n",
    "def get_edge_color(edge):\n",
    "    \"\"\"Color de aristas.\"\"\"\n",
    "    return '#666666'\n",
    "\n",
    "# --- PASO 9: CREAR VISUALIZACIÓN CON MATPLOTLIB (MEJORADA) ---\n",
    "if ps.number_of_nodes() > 0 and ps.number_of_edges() > 0:\n",
    "    plt.figure(figsize=(20, 16))\n",
    "\n",
    "    # USAR LAYOUT FORCE-DIRECTED\n",
    "    pos = nx.spring_layout(ps, k=1/np.sqrt(ps.number_of_nodes()), iterations=100, seed=42)\n",
    "\n",
    "    print(f\"Usando layout spring con {ps.number_of_nodes()} nodos y {ps.number_of_edges()} aristas\")\n",
    "\n",
    "    # Preparar colores y tamaños de nodos\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "\n",
    "    for node in ps.nodes():\n",
    "        node_attrs = ps.nodes[node]\n",
    "        ministerio = node_attrs.get('ministerio', 'Sin clasificar')\n",
    "        color = color_map_ministerios.get(ministerio, 'grey')\n",
    "\n",
    "        ponderacion_valor = node_attrs.get('ponderación', min_ponderacion)\n",
    "        if max_ponderacion != min_ponderacion:\n",
    "            normalized = (ponderacion_valor - min_ponderacion) / (max_ponderacion - min_ponderacion)\n",
    "            size = 500 + normalized * 3000 # Tamaños: 500-3500\n",
    "        else:\n",
    "            size = 1500\n",
    "\n",
    "        node_colors.append(color)\n",
    "        node_sizes.append(size)\n",
    "\n",
    "    # Calcular anchos de aristas basados en peso\n",
    "    edge_weights = []\n",
    "    for u, v, data in ps.edges(data=True):\n",
    "        weight = data.get('weight', 0.5)\n",
    "        width = 0.5 + weight * 3.5 # Convertir peso a ancho visual (0.5 a 4.0)\n",
    "        edge_weights.append(width)\n",
    "\n",
    "    # Dibujar la red con más detalle\n",
    "    nx.draw_networkx_nodes(ps, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                           alpha=0.8, linewidths=1, edgecolors='black')\n",
    "    nx.draw_networkx_edges(ps, pos, alpha=0.6, width=edge_weights, edge_color='#666666')\n",
    "    nx.draw_networkx_labels(ps, pos, font_size=8, font_weight='bold', font_color='white')\n",
    "\n",
    "    plt.title(\"Red de Fuentes Normativas\\n(Tamaño = Ponderación, Color = Ministerio, Grosor = Peso de Conexión)\",\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"red_fuentes_normativas.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"¡Gráfico guardado como 'red_fuentes_normativas.png'!\")\n",
    "else:\n",
    "    print(\"No se puede crear la visualización: la red no tiene nodos o aristas.\")\n",
    "\n",
    "# --- PASO 10: VISUALIZACIÓN INTERACTIVA (MEJORADA) ---\n",
    "def mostrar_visualizacion_interactiva():\n",
    "    \"\"\"Función separada para mostrar la visualización interactiva\"\"\"\n",
    "    try:\n",
    "\n",
    "        if ps.number_of_nodes() > 0:\n",
    "            w = GraphWidget(graph=ps)\n",
    "\n",
    "            def get_edge_thickness(edge: dict):\n",
    "                weight = edge.get('properties', {}).get('weight', 0)\n",
    "                return 1 + (weight * 7)\n",
    "\n",
    "            w.set_node_scale_factor_mapping(get_node_scale_factor)\n",
    "            w.set_node_label_mapping(get_node_label_style)\n",
    "            w.set_node_color_mapping(get_node_color)\n",
    "            w.set_edge_color_mapping(get_edge_color)\n",
    "            w.set_edge_thickness_factor_mapping(get_edge_thickness)\n",
    "            w.set_sidebar(start_with='Neighborhood')\n",
    "\n",
    "            print(\"\\n¡Visualización interactiva creada!\")\n",
    "            print(\"Para mostrarla, ejecuta: display(w)\")\n",
    "            return w\n",
    "        else:\n",
    "            print(\"No se puede crear visualización interactiva: la red no tiene nodos.\")\n",
    "            return None\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\nPaquete yfiles_jupyter_graphs no disponible.\")\n",
    "        print(\"Instálalo con: pip install yfiles_jupyter_graphs\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError creando visualización interactiva: {e}\")\n",
    "        return None\n",
    "\n",
    "# Crear la visualización interactiva pero no mostrarla automáticamente\n",
    "w = mostrar_visualizacion_interactiva()\n",
    "if w is not None:\n",
    "    globals()['w'] = w \n",
    "\n",
    "# --- PASO 11: ESTADÍSTICAS DE LA RED ---\n",
    "print(f\"\\n=== ESTADÍSTICAS DE LA RED ===\")\n",
    "print(f\"Número de fuentes normativas (nodos): {ps.number_of_nodes()}\")\n",
    "print(f\"Número de conexiones (aristas): {ps.number_of_edges()}\")\n",
    "\n",
    "if ps.number_of_nodes() > 1 and ps.number_of_edges() > 0:\n",
    "    print(f\"Densidad de la red: {nx.density(ps):.4f}\")\n",
    "\n",
    "    if nx.is_connected(ps):\n",
    "        print(f\"Diámetro de la red: {nx.diameter(ps)}\")\n",
    "        print(f\"Longitud promedio de camino: {nx.average_shortest_path_length(ps):.4f}\")\n",
    "    else:\n",
    "        print(\"La red no está completamente conectada.\")\n",
    "        print(f\"Número de componentes conectados: {nx.number_connected_components(ps)}\")\n",
    "\n",
    "    # Nodos más centrales\n",
    "    centralidad = nx.degree_centrality(ps)\n",
    "    top_centrales = sorted(centralidad.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(f\"\\n=== TOP 10 FUENTES MÁS CENTRALES ===\")\n",
    "    for i, (fuente, centralidad_val) in enumerate(top_centrales, 1):\n",
    "        node_attrs = ps.nodes[fuente]\n",
    "        ministerio = node_attrs.get('ministerio', 'N/A')\n",
    "        ponderacion_val = node_attrs.get('ponderación', 'N/A')\n",
    "        print(f\"{i:2d}. {str(fuente)[:50]:<50} | Ministerio: {ministerio} | Centralidad: {centralidad_val:.3f} | Ponderación: {ponderacion_val}\")\n",
    "\n",
    "# Distribución por ministerio\n",
    "print(f\"\\n=== DISTRIBUCIÓN POR MINISTERIO EN LA RED ===\")\n",
    "ministerios_en_red = {}\n",
    "for node in ps.nodes():\n",
    "    ministerio = ps.nodes[node].get('ministerio', 'Sin clasificar')\n",
    "    ministerios_en_red[ministerio] = ministerios_en_red.get(ministerio, 0) + 1\n",
    "\n",
    "for ministerio, count in sorted(ministerios_en_red.items(), key=lambda x: x[1], reverse=True):\n",
    "    color = color_map_ministerios.get(ministerio, 'grey')\n",
    "    print(f\"{ministerio}: {count} fuentes (Color: {color})\")\n",
    "\n",
    "# --- FUNCIONES AUXILIARES PARA USO POSTERIOR ---\n",
    "def mostrar_con_layout(layout_tipo='spring'):\n",
    "    \"\"\"Función para probar diferentes layouts\"\"\"\n",
    "    if ps.number_of_nodes() == 0:\n",
    "        print(\"No hay nodos en la red\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(20, 16))\n",
    "\n",
    "    if layout_tipo == 'kamada_kawai':\n",
    "        pos = nx.kamada_kawai_layout(ps)\n",
    "    elif layout_tipo == 'fruchterman':\n",
    "        pos = nx.fruchterman_reingold_layout(ps, k=1, iterations=100)\n",
    "    elif layout_tipo == 'circular':\n",
    "        pos = nx.circular_layout(ps)\n",
    "    else: \n",
    "        pos = nx.spring_layout(ps, k=1/np.sqrt(ps.number_of_nodes()), iterations=100, seed=42)\n",
    "\n",
    "    node_colors = [color_map_ministerios.get(ps.nodes[node].get('ministerio', 'Sin clasificar'), 'grey') for node in ps.nodes()]\n",
    "    node_sizes = []\n",
    "    for node in ps.nodes():\n",
    "        node_attrs = ps.nodes[node]\n",
    "        ponderacion_valor = node_attrs.get('ponderación', min_ponderacion)\n",
    "        if max_ponderacion != min_ponderacion:\n",
    "            normalized = (ponderacion_valor - min_ponderacion) / (max_ponderacion - min_ponderacion)\n",
    "            size = 500 + normalized * 3000\n",
    "        else:\n",
    "            size = 1500\n",
    "        node_sizes.append(size)\n",
    "\n",
    "    nx.draw_networkx_nodes(ps, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                           alpha=0.8, linewidths=1, edgecolors='black')\n",
    "    nx.draw_networkx_edges(ps, pos, alpha=0.6, width=1, edge_color='#666666')\n",
    "    nx.draw_networkx_labels(ps, pos, font_size=8, font_weight='bold', font_color='white')\n",
    "\n",
    "    plt.title(f\"Red de Fuentes Normativas - Layout: {layout_tipo}\",\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Mostrando red con layout: {layout_tipo}\")\n",
    "\n",
    "def cambiar_umbral(nuevo_umbral):\n",
    "    \"\"\"Función para recrear la red con diferente umbral\"\"\"\n",
    "    global ps\n",
    "\n",
    "    print(f\"Recreando red con umbral: {nuevo_umbral}\")\n",
    "    ps_nuevo = nx.Graph()\n",
    "\n",
    "    for u, v, w in mst.edges(data=True):\n",
    "        ps_nuevo.add_edge(u, v, **{'weight': w[\"weight\"]})\n",
    "\n",
    "    aristas_agregadas = 0\n",
    "    for u, v, w in g.edges(data=True):\n",
    "        if w['weight'] >= nuevo_umbral:\n",
    "            ps_nuevo.add_edge(u, v, **{'weight': w[\"weight\"]})\n",
    "            aristas_agregadas += 1\n",
    "\n",
    "    for node_id in ps_nuevo.nodes():\n",
    "        if node_id in ponderacion_dict:\n",
    "            node_attributes = ponderacion_dict[node_id]\n",
    "            nx.set_node_attributes(ps_nuevo, {node_id: node_attributes})\n",
    "        else:\n",
    "            nx.set_node_attributes(ps_nuevo, {node_id: {\n",
    "                'ministerio': 'Sin clasificar',\n",
    "                'ponderación': 1\n",
    "            }})\n",
    "\n",
    "    ps = ps_nuevo\n",
    "    print(f\"Nueva red: {ps.number_of_nodes()} nodos, {ps.number_of_edges()} aristas\")\n",
    "    print(f\"Aristas agregadas por umbral: {aristas_agregadas}\")\n",
    "    mostrar_con_layout('spring')\n",
    "    return ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656527fe-8feb-449b-b839-045b63565ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_network(ps, pos, ponderacion_dict, color_map_ministerios):\n",
    "    \"\"\"\n",
    "    Crea una visualización interactiva tipo network_plot de R usando Plotly\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preparar datos de aristas\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_info = []\n",
    "    \n",
    "    for u, v, data in ps.edges(data=True):\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        # Información de hover para aristas\n",
    "        weight = data.get('weight_normalized', 0)\n",
    "        edge_info.append(f\"Conexión: {u} ↔ {v}<br>Similitud: {weight:.3f}\")\n",
    "    \n",
    "    # Crear trazado de aristas\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.8, color='rgba(125, 125, 125, 0.5)'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Preparar datos de nodos por ministerio\n",
    "    ministerio_traces = {}\n",
    "    \n",
    "    for node in ps.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_data = ps.nodes[node]\n",
    "        ministerio = node_data.get('ministerio', 'Sin clasificar')\n",
    "        ponderacion = node_data.get('ponderación', 1)\n",
    "        \n",
    "        # Calcular tamaño del nodo\n",
    "        min_pond = min([ps.nodes[n].get('ponderación', 1) for n in ps.nodes()])\n",
    "        max_pond = max([ps.nodes[n].get('ponderación', 1) for n in ps.nodes()])\n",
    "        \n",
    "        if max_pond > min_pond:\n",
    "            normalized_size = (ponderacion - min_pond) / (max_pond - min_pond)\n",
    "            size = 15 + normalized_size * 40\n",
    "        else:\n",
    "            size = 25\n",
    "            \n",
    "        # Calcular centralidad\n",
    "        centrality = nx.degree_centrality(ps)[node]\n",
    "        \n",
    "        # Información de hover\n",
    "        hover_text = f\"\"\"<b>{node}</b><br>\n",
    "        Ministerio: {ministerio}<br>\n",
    "        Ponderación: {ponderacion}<br>\n",
    "        Centralidad: {centrality:.3f}<br>\n",
    "        Conexiones: {ps.degree(node)}\"\"\"\n",
    "        \n",
    "        # Agrupar por ministerio\n",
    "        if ministerio not in ministerio_traces:\n",
    "            ministerio_traces[ministerio] = {\n",
    "                'x': [], 'y': [], 'text': [], 'size': [], 'hover': [], 'color': color_map_ministerios.get(ministerio, '#838D9B')\n",
    "            }\n",
    "        \n",
    "        ministerio_traces[ministerio]['x'].append(x)\n",
    "        ministerio_traces[ministerio]['y'].append(y)\n",
    "        ministerio_traces[ministerio]['text'].append(node.replace(' ', '<br>'))\n",
    "        ministerio_traces[ministerio]['size'].append(size)\n",
    "        ministerio_traces[ministerio]['hover'].append(hover_text)\n",
    "    \n",
    "    # Crear trazados por ministerio (para leyenda automática)\n",
    "    traces = [edge_trace]\n",
    "    \n",
    "    for ministerio, data in ministerio_traces.items():\n",
    "        trace = go.Scatter(\n",
    "            x=data['x'], y=data['y'],\n",
    "            mode='markers+text',\n",
    "            text=data['text'],\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(size=10, color='white', family='Arial Black'),\n",
    "            hovertext=data['hover'],\n",
    "            hoverinfo='text',\n",
    "            name=ministerio,\n",
    "            marker=dict(\n",
    "                size=data['size'],\n",
    "                color=data['color'],\n",
    "                line=dict(width=2, color='rgba(50, 50, 50, 0.8)'),\n",
    "                opacity=0.9\n",
    "            ),\n",
    "            showlegend=True\n",
    "        )\n",
    "        traces.append(trace)\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = go.Figure(data=traces)\n",
    "    \n",
    "    # Configurar layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text='Red de Fuentes Normativas - Visualización Interactiva',\n",
    "            x=0.5,\n",
    "            font=dict(size=24, family='Arial', color='#1C1C1C')\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"middle\",\n",
    "            y=0.5,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "            font=dict(size=12),\n",
    "            bgcolor=\"rgba(255,255,255,0.9)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=20, l=5, r=200, t=60),\n",
    "        annotations=[\n",
    "            dict(\n",
    "                text=\"Tamaño = Ponderación | Hover para detalles\",\n",
    "                showarrow=False,\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                x=0.005, y=0.99,\n",
    "                font=dict(color='gray', size=12)\n",
    "            )\n",
    "        ],\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        width=1400,\n",
    "        height=900\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Crear y mostrar la visualización interactiva\n",
    "if ps.number_of_nodes() > 0:\n",
    "    print(\"\\nCreando visualización interactiva con Plotly...\")\n",
    "    \n",
    "    fig_interactive = create_interactive_network(ps, pos, ponderacion_dict, color_map_ministerios)\n",
    "    \n",
    "    # Guardar como HTML\n",
    "    plot(fig_interactive, filename='red_fuentes_interactiva.html', auto_open=True)\n",
    "    print(\"Visualización interactiva guardada como 'red_fuentes_interactiva.html'\")\n",
    "\n",
    "# Función adicional para crear gráfico con filtros dinámicos\n",
    "def create_filterable_network(ps, pos, ponderacion_dict, color_map_ministerios):\n",
    "    \"\"\"\n",
    "    Crea una versión con controles deslizantes para filtrar por ponderación\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener rango de ponderaciones\n",
    "    ponderaciones = [ps.nodes[node].get('ponderación', 1) for node in ps.nodes()]\n",
    "    min_pond, max_pond = min(ponderaciones), max(ponderaciones)\n",
    "    \n",
    "    # Crear steps para el slider\n",
    "    steps = []\n",
    "    for i in range(11):  # 11 pasos (0-10)\n",
    "        threshold = min_pond + (max_pond - min_pond) * i / 10\n",
    "        \n",
    "        # Determinar qué nodos mostrar\n",
    "        visible_nodes = [node for node in ps.nodes() \n",
    "                        if ps.nodes[node].get('ponderación', 1) >= threshold]\n",
    "        \n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\n",
    "                \"visible\": [True] + [ministerio in [ps.nodes[node].get('ministerio', 'Sin clasificar') \n",
    "                                                  for node in visible_nodes] \n",
    "                                   for ministerio in color_map_ministerios.keys()]\n",
    "            }],\n",
    "            label=f\"≥{threshold:.2f}\"\n",
    "        )\n",
    "        steps.append(step)\n",
    "    \n",
    "    # Crear figura base\n",
    "    fig = create_interactive_network(ps, pos, ponderacion_dict, color_map_ministerios)\n",
    "    \n",
    "    # Añadir slider\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Ponderación mínima: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "    \n",
    "    fig.update_layout(sliders=sliders)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Crear versión con filtros\n",
    "if ps.number_of_nodes() > 0:\n",
    "    print(\"Creando visualización con controles de filtrado...\")\n",
    "    \n",
    "    fig_filterable = create_filterable_network(ps, pos, ponderacion_dict, color_map_ministerios)\n",
    "    plot(fig_filterable, filename='red_fuentes_filtrable.html', auto_open=False)\n",
    "    print(\"Visualización filtrable guardada como 'red_fuentes_filtrable.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df82cc9-5e64-4347-9a03-c9e17f1b3945",
   "metadata": {},
   "source": [
    "### Y Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80644b50-e656-4b0b-8a31-5667ece7eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_test = nx.complete_graph(5)\n",
    "w_test = GraphWidget(graph=G_test)\n",
    "display(w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd94ee5-52cc-4a01-9186-84716fe0dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = GraphWidget(graph=ps)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d98353-dd30-40d5-834d-a1dca1fe2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ps2' not in locals() or ps2.number_of_nodes() == 0:\n",
    "    print(\"Error: El objeto de grafo 'ps' no existe o está vacío.\")\n",
    "    print(\"Por favor, ejecuta el Bloque 1 primero para crear el grafo.\")\n",
    "else:\n",
    "    \n",
    "    def get_node_scale_factor(node: Dict) -> float:\n",
    "        \"\"\"Calcula el factor de escala del nodo basado en su 'ponderación'.\"\"\"\n",
    "        ponderacion_valor = node.get('properties', {}).get('ponderación', min_ponderacion)\n",
    "        if max_ponderacion > min_ponderacion:\n",
    "            normalized = (ponderacion_valor - min_ponderacion) / (max_ponderacion - min_ponderacion)\n",
    "            return 1.0 + normalized * 4.0  \n",
    "        return 2.5 \n",
    "\n",
    "    def get_node_label_style(node: Dict) -> Dict:\n",
    "        \"\"\"Define el estilo y contenido de la etiqueta del nodo.\"\"\"\n",
    "        fuente_nombre = node.get('id', '')\n",
    "        return {\n",
    "            'text': fuente_nombre,\n",
    "            'wrapping': 'word',\n",
    "            'maximumWidth': 120,\n",
    "            'textAlignment': 'center',\n",
    "            'fontSize': 10\n",
    "        }\n",
    "\n",
    "    def get_node_color(node: Dict) -> str:\n",
    "        \"\"\"Obtiene el color del nodo basado en su ministerio.\"\"\"\n",
    "        ministerio = node.get('properties', {}).get('ministerio', 'Sin clasificar')\n",
    "        return color_map_ministerios.get(ministerio, '#cccccc') # Usa el mapa de colores del Bloque 1\n",
    "\n",
    "    def get_edge_thickness(edge: Dict) -> float:\n",
    "        \"\"\"Define el grosor de la arista basado en su 'weight'.\"\"\"\n",
    "        weight = edge.get('properties', {}).get('weight', 0)\n",
    "        return 1.0 + (weight * 7.0) # Grosor de 1 a 8\n",
    "\n",
    "    def get_edge_color(edge: Dict) -> str:\n",
    "        \"\"\"Define el color de las aristas.\"\"\"\n",
    "        return '#888888'\n",
    "\n",
    "    print(\"\\nGenerando visualización interactiva con yFiles...\")\n",
    "    \n",
    "    w = GraphWidget(graph=ps2)\n",
    "\n",
    "    w.set_node_scale_factor_mapping(get_node_scale_factor)\n",
    "    w.set_node_color_mapping(get_node_color)\n",
    "    w.set_edge_thickness_factor_mapping(get_edge_thickness)\n",
    "    w.set_edge_color_mapping(get_edge_color)\n",
    "    w.set_sidebar(start_with='Neighborhood')\n",
    "\n",
    "    print(\"¡Visualización interactiva lista! Se mostrará a continuación.\")\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed78283-5fbd-4e80-a74c-ca2aa0c6fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.set_node_scale_factor_mapping(get_node_scale_factor)\n",
    "w.set_node_label_mapping(get_node_label_style)\n",
    "w.set_node_color_mapping(get_node_color) \n",
    "w.set_edge_color_mapping(get_edge_color) \n",
    "w.set_sidebar(start_with='Neighborhood')\n",
    "display(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
